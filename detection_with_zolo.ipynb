{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('/Users/suryanshpatel/Projects/Pose_detection/yolo/yolov3.weights', '/Users/suryanshpatel/Projects/Pose_detection/yolo/yolov3.cfg')\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "\n",
    "# Adjust indexing to handle both cases where net.getUnconnectedOutLayers() returns a 1D or 2D array\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718565997.649749 4016892 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1718565997.799171 4017120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1718565997.805977 4017117 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Function to compare joint points\n",
    "def compare_joint_points(user_points, professional_points):\n",
    "    differences = []\n",
    "    for user_point, professional_point in zip(user_points, professional_points):\n",
    "        diff = np.linalg.norm(np.array(user_point) - np.array(professional_point))\n",
    "        differences.append(diff)\n",
    "    return differences\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suryanshpatel/Projects/Pose_detection/.venv/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406,) (406, 4)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "all_joint_points = []\n",
    "best_box_dimensions = []\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Track the best detection\n",
    "    best_confidence = 0\n",
    "    best_box = None\n",
    "\n",
    "    # Get bounding box for the person with the highest confidence\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > best_confidence:  # Class 0 is for person\n",
    "                best_confidence = confidence\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = max(0, int(center_x - w / 2))\n",
    "                y = max(0, int(center_y - h / 2))\n",
    "                w = min(w, width - x)\n",
    "                h = min(h, height - y)\n",
    "                best_box = (x, y, w, h)\n",
    "\n",
    "    if best_box:\n",
    "        x, y, w, h = best_box\n",
    "\n",
    "        # Extract the region within the bounding box\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        if roi.size == 0:  # Check if the ROI is empty\n",
    "            continue\n",
    "\n",
    "        # Process the region with MediaPipe for pose estimation\n",
    "        rgb_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Capture joint points\n",
    "        joint_points = []\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                joint_points.append((landmark.x * w + x, landmark.y * h + y))\n",
    "\n",
    "        \n",
    "        #Surya - don't want to normalize now store as it is as we are storing box dimentions as well\n",
    "        #  Normalize joint points within bounding box\n",
    "        # normalized_points = normalize_joint_points_within_box(joint_points, (x, y, w, h))\n",
    "        # all_joint_points.append(normalized_points)\n",
    "        \n",
    "        \n",
    "        all_joint_points.append(joint_points)\n",
    "        best_box_dimensions.append((x, y, w, h))\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the pose annotation on the image\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(roi, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Optionally display the image\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points , dtype=object)\n",
    "best_box_dimensions = np.array(best_box_dimensions)\n",
    "\n",
    "print(all_joint_points.shape , best_box_dimensions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_box_dimensions = np.array(best_box_dimensions)\n",
    "best_box_dimensions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load professional's joint points (normalized) for comparison\n",
    "# professional_joint_points = np.load('professional_joint_points.npy')\n",
    "\n",
    "# Compare user's joint points with professional's\n",
    "# differences = []\n",
    "# for user_points in all_joint_points:\n",
    "#     differences.append(compare_joint_points(user_points, professional_joint_points))\n",
    "\n",
    "# # Convert differences to NumPy array for easier processing\n",
    "# differences = np.array(differences)\n",
    "\n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i, joint_diff in enumerate(differences.T):\n",
    "#     plt.plot(joint_diff, label=f'Joint {i+1}')\n",
    "# plt.xlabel('Frame')\n",
    "# plt.ylabel('Difference')\n",
    "# plt.title('Joint Differences between User and Professional')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(352.6430822610855, 315.939789891243),\n",
       " (368.44150614738464, 303.62629356980324),\n",
       " (369.49876606464386, 304.2768477201462),\n",
       " (373.27290737628937, 307.6255923807621),\n",
       " (354.05673015117645, 304.817487180233),\n",
       " (351.71358346939087, 301.469540476799),\n",
       " (349.4052257537842, 298.1421529054642),\n",
       " (363.2787981033325, 322.73301842808723),\n",
       " (339.75286316871643, 291.79549875855446),\n",
       " (349.47095239162445, 323.88185772299767),\n",
       " (341.1703305244446, 313.4078561067581),\n",
       " (361.00844407081604, 360.02842622995377),\n",
       " (286.57259702682495, 276.7686038017273),\n",
       " (397.65850853919983, 363.7676848769188),\n",
       " (301.2435783147812, 227.27840754389763),\n",
       " (391.50967812538147, 347.5298194885254),\n",
       " (317.4966777563095, 255.740179002285),\n",
       " (389.32532572746277, 353.32173693180084),\n",
       " (316.1062331199646, 264.395151168108),\n",
       " (382.24276328086853, 339.88964545726776),\n",
       " (324.36191260814667, 272.4565527141094),\n",
       " (381.95830857753754, 335.1317709982395),\n",
       " (326.0780940055847, 270.24686846137047),\n",
       " (293.9131643772125, 421.3800995349884),\n",
       " (242.79162204265594, 374.8731991946697),\n",
       " (382.79661762714386, 363.9206423163414),\n",
       " (311.6868885755539, 302.29090440273285),\n",
       " (432.85844922065735, 324.76413238048553),\n",
       " (321.56711411476135, 250.11525365710258),\n",
       " (425.47723639011383, 334.49290457367897),\n",
       " (308.0495159626007, 264.82501539587975),\n",
       " (472.26349425315857, 281.37341129779816),\n",
       " (363.1689248085022, 195.19758734107018)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_joint_points[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
