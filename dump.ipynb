{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic boiler plate code don't run this tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "cap = cv2.VideoCapture('/Users/suryanshpatel/Projects/Pose_detection/correct-squat-side-view.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Convert the image back to BGR for rendering\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw the pose annotation on the image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Pose Detection', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trash plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_joint_points(image, results):\n",
    "    joint_points = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            joint_points.append((landmark.x, landmark.y))\n",
    "    return joint_points\n",
    "\n",
    "def plot_joint_points(all_joint_points):\n",
    "    for i, points in enumerate(zip(*all_joint_points)):\n",
    "        xs, ys = zip(*points)\n",
    "        plt.figure()\n",
    "        plt.plot(xs, ys, marker='o', linestyle='-')\n",
    "        plt.title(f'Joint {i} Motion Over Time')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "all_joint_points = [] \n",
    "\n",
    "for frame in key_frames:\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    joint_points = capture_joint_points(frame, results)\n",
    "    all_joint_points.append(joint_points)\n",
    "\n",
    "# plot_joint_points(all_joint_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only displaying one frame may be vs code problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Function to animate the joint points\n",
    "def animate_joint_points(all_joint_points):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)  # Invert y-axis to match image coordinates\n",
    "\n",
    "    # Initialize plot\n",
    "    scat = ax.scatter([], [], c='blue')\n",
    "\n",
    "    def update(frame):\n",
    "        joint_points = all_joint_points[frame]\n",
    "        xs, ys = zip(*joint_points)\n",
    "        scat.set_offsets(np.c_[xs, ys])\n",
    "        ax.set_title(f'Frame {frame}')\n",
    "        return scat,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(all_joint_points), blit=True, repeat=False)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to animate joint points\n",
    "animate_joint_points(all_joint_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames coodinated scaled\n",
    "def plot_coordinates(coordinates, frame_number, save_folder):\n",
    "    plt.figure()\n",
    "    xs, ys = zip(*coordinates)\n",
    "    plt.plot(xs, ys, marker='o', linestyle='-')\n",
    "    plt.title(f'Joint Coordinates - Frame {frame_number}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.savefig(os.path.join(save_folder, f'coordinates_frame_{frame_number:03d}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view.mp4'\n",
    "output_folder = '/Users/suryanshpatel/Projects/Pose_detection/annotated_frames'\n",
    "coordinates_folder = '/Users/suryanshpatel/Projects/Pose_detection/coordinates'\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(coordinates_folder, exist_ok=True)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter for annotated video\n",
    "output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view-skeleton2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize variables for key frames\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Create a blank image (white background)\n",
    "    blank_image = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Draw the pose annotation on the blank image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Save annotated frame\n",
    "        annotated_frame_path = os.path.join(output_folder, f'frame_{len(key_frames):03d}.png')\n",
    "        cv2.imwrite(annotated_frame_path, cv2.cvtColor(blank_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Optionally plot and save coordinates\n",
    "        if results.pose_landmarks:\n",
    "            joint_coordinates = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]\n",
    "            plot_coordinates(joint_coordinates, frame_number=len(key_frames), save_folder=coordinates_folder)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(blank_image)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')\n",
    "print(f'Annotated frames saved in {output_folder}')\n",
    "print(f'Coordinate plots saved in {coordinates_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below code is to save skeleton video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect-squat-side-view.mp4'#'/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view.mp4'\n",
    "output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view-skeleton3.mp4'\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 files\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Create a blank image (white background)\n",
    "    blank_image = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Draw the pose annotation on the blank image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(blank_image)\n",
    "\n",
    "    # Optionally display the image (comment out if not needed)\n",
    "    # cv2.imshow('Pose Detection', blank_image)\n",
    "    # if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "    #     break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO capture Video from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "systematic video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def initialize_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    return cap, frame_width, frame_height, fps\n",
    "\n",
    "def initialize_video_writer(output_video_path, frame_width, frame_height, fps):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    return out\n",
    "\n",
    "def is_key_frame(prev_frame, current_frame):\n",
    "    # Implement your logic to determine key frames\n",
    "    return False  # Placeholder for demo\n",
    "\n",
    "def annotate_frame(frame, results):\n",
    "    blank_image = np.ones_like(frame) * 255  # White background\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    return blank_image\n",
    "\n",
    "def save_frame(frame, output_folder, frame_number):\n",
    "    annotated_frame_path = os.path.join(output_folder, f'frame_{frame_number:03d}.png')\n",
    "    cv2.imwrite(annotated_frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def plot_coordinates(coordinates, frame_number, frame_width, frame_height, save_folder):\n",
    "    plt.figure()\n",
    "    scaled_coordinates = [(x * frame_width, y * frame_height) for x, y in coordinates]\n",
    "    xs, ys = zip(*scaled_coordinates)\n",
    "    plt.plot(xs, ys, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Joint Coordinates - Frame {frame_number}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.xlim(0, frame_width)\n",
    "    plt.ylim(frame_height, 0)  # Invert y-axis limits\n",
    "    plt.savefig(os.path.join(save_folder, f'coordinates_frame_{frame_number:03d}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def process_video(input_video_path, output_folder, coordinates_folder, output_video_path):\n",
    "    # Create output folders if they don't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(coordinates_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize video capture\n",
    "    cap, frame_width, frame_height, fps = initialize_video_capture(input_video_path)\n",
    "\n",
    "    # Initialize VideoWriter for annotated video\n",
    "    out = initialize_video_writer(output_video_path, frame_width, frame_height, fps)\n",
    "\n",
    "    # Initialize variables for key frames\n",
    "    key_frames = []\n",
    "    prev_frame = None\n",
    "    frame_number = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "            key_frames.append(frame)\n",
    "            prev_frame = gray_frame\n",
    "\n",
    "        # Process the image and detect the pose\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Annotate and save the frame\n",
    "        annotated_frame = annotate_frame(frame, results)\n",
    "        save_frame(annotated_frame, output_folder, frame_number)\n",
    "\n",
    "        # Plot and save coordinates\n",
    "        if results.pose_landmarks:\n",
    "            joint_coordinates = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]\n",
    "            plot_coordinates(joint_coordinates, frame_number, frame_width, frame_height, coordinates_folder)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    # cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "    print(f'Output video saved as {output_video_path}')\n",
    "    print(f'Annotated frames saved in {output_folder}')\n",
    "    print(f'Coordinate plots saved in {coordinates_folder}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect-squat-side-view.mp4'\n",
    "    output_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/annotated_frames/af5'\n",
    "    coordinates_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/plots/co5'\n",
    "    output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect_5.mp4'\n",
    "    process_video(input_video_path, output_folder, coordinates_folder, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to cut video on exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def is_key_frame(prev_frame, current_frame, threshold=30):\n",
    "    diff = cv2.absdiff(prev_frame, current_frame)\n",
    "    non_zero_count = np.count_nonzero(diff)\n",
    "    return non_zero_count > threshold\n",
    "\n",
    "def is_exercising(pose_landmarks, model):\n",
    "    # Convert pose landmarks to a format suitable for the model\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in pose_landmarks.landmark]).flatten()\n",
    "    landmarks = landmarks.reshape(1, -1)  # Reshape for the model\n",
    "    prediction = model.predict(landmarks)\n",
    "    return prediction[0] == 1  # Assuming the model returns 1 for exercise and 0 for non-exercise\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_frame = None\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the pre-trained model (replace with your actual model loading code)\n",
    "model = tf.keras.models.load_model('path_to_your_model.h5')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 30.0  # Assuming a frame rate of 30 FPS\n",
    "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "recording = False\n",
    "video_writer = None\n",
    "segment_count = 0\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "            results = pose.process(rgb_frame)\n",
    "            if results.pose_landmarks and is_exercising(results.pose_landmarks, model):\n",
    "                if not recording:\n",
    "                    # Start a new video segment\n",
    "                    recording = True\n",
    "                    segment_count += 1\n",
    "                    video_writer = cv2.VideoWriter(f'exercise_segment_{segment_count}.avi', fourcc, fps, frame_size)\n",
    "                \n",
    "                # Write the frame to the video file\n",
    "                video_writer.write(frame)\n",
    "            else:\n",
    "                if recording:\n",
    "                    # Stop recording the current video segment\n",
    "                    recording = False\n",
    "                    video_writer.release()\n",
    "                    video_writer = None\n",
    "\n",
    "            prev_frame = gray_frame\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Key Frame Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video writer if recording\n",
    "if recording and video_writer is not None:\n",
    "    video_writer.release()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to detect multiple people and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# Adjust indexing to handle both cases where net.getUnconnectedOutLayers() returns a 1D or 2D array\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Function to normalize joint points within bounding box\n",
    "def normalize_joint_points_within_box(joint_points, box):\n",
    "    x_min, y_min, width, height = box\n",
    "    normalized_points = []\n",
    "    for (x, y) in joint_points:\n",
    "        norm_x = (x - x_min) / width\n",
    "        norm_y = (y - y_min) / height\n",
    "        normalized_points.append((norm_x, norm_y))\n",
    "    return normalized_points\n",
    "\n",
    "# Function to compare joint points\n",
    "def compare_joint_points(user_points, professional_points):\n",
    "    differences = []\n",
    "    for user_point, professional_point in zip(user_points, professional_points):\n",
    "        diff = np.linalg.norm(np.array(user_point) - np.array(professional_point))\n",
    "        differences.append(diff)\n",
    "    return differences\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "all_joint_points = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Get bounding box for the person\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > 0.5:  # Class 0 is for person\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = max(0, int(center_x - w / 2))\n",
    "                y = max(0, int(center_y - h / 2))\n",
    "                w = min(w, width - x)\n",
    "                h = min(h, height - y)\n",
    "\n",
    "                # Extract the region within the bounding box\n",
    "                roi = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                if roi.size == 0:  # Check if the ROI is empty\n",
    "                    continue\n",
    "\n",
    "                # Process the region with MediaPipe for pose estimation\n",
    "                rgb_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(rgb_frame)\n",
    "\n",
    "                # Capture joint points\n",
    "                joint_points = []\n",
    "                if results.pose_landmarks:\n",
    "                    for landmark in results.pose_landmarks.landmark:\n",
    "                        joint_points.append((landmark.x * w + x, landmark.y * h + y))\n",
    "\n",
    "                # Normalize joint points within bounding box\n",
    "                normalized_points = normalize_joint_points_within_box(joint_points, (x, y, w, h))\n",
    "                all_joint_points.append(normalized_points)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw the pose annotation on the image\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(roi, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Optionally display the image\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points)\n",
    "\n",
    "# Load professional's joint points (normalized) for comparison\n",
    "professional_joint_points = np.load('professional_joint_points.npy')\n",
    "\n",
    "# Compare user's joint points with professional's\n",
    "differences = []\n",
    "for user_points in all_joint_points:\n",
    "    differences.append(compare_joint_points(user_points, professional_joint_points))\n",
    "\n",
    "# Convert differences to NumPy array for easier processing\n",
    "differences = np.array(differences)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, joint_diff in enumerate(differences.T):\n",
    "    plt.plot(joint_diff, label=f'Joint {i+1}')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Difference')\n",
    "plt.title('Joint Differences between User and Professional')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working code with yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "all_joint_points = []\n",
    "best_box_dimensions = []\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 files\n",
    "# output = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Track the best detection\n",
    "    best_confidence = 0\n",
    "    best_box = None\n",
    "\n",
    "    # Get bounding box for the person with the highest confidence\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > best_confidence:  # Class 0 is for person\n",
    "                best_confidence = confidence\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = max(0, int(center_x - w / 2))\n",
    "                y = max(0, int(center_y - h / 2))\n",
    "                w = min(w, width - x)\n",
    "                h = min(h, height - y)\n",
    "                best_box = (x, y, w, h)\n",
    "\n",
    "    if best_box:\n",
    "        x, y, w, h = best_box\n",
    "\n",
    "        # Extract the region within the bounding box\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        if roi.size == 0:  # Check if the ROI is empty\n",
    "            continue\n",
    "\n",
    "        # Process the region with MediaPipe for pose estimation\n",
    "        rgb_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Capture joint points\n",
    "        joint_points = []\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                joint_points.append((landmark.x * w + x, landmark.y * h + y))\n",
    "\n",
    "        \n",
    "        #Surya - don't want to normalize now store as it is as we are storing box dimentions as well\n",
    "        #  Normalize joint points within bounding box\n",
    "        # normalized_points = normalize_joint_points_within_box(joint_points, (x, y, w, h))\n",
    "        # all_joint_points.append(normalized_points)\n",
    "        \n",
    "        \n",
    "        all_joint_points.append(joint_points)\n",
    "        best_box_dimensions.append((x, y, w, h))\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the pose annotation on the image\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(roi, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    # output.write(frame)\n",
    "\n",
    "    # Optionally display the image\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "# output.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points , dtype=object)\n",
    "best_box_dimensions = np.array(best_box_dimensions)\n",
    "\n",
    "print(all_joint_points.shape , best_box_dimensions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plots every joint points not adding arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example function to extract data from all_frame_data\n",
    "def extract_joint_data(all_frame_data):\n",
    "    num_frames = all_frame_data.shape[0]\n",
    "    num_joints = len(all_frame_data[0])  # Assuming all frames have the same number of joints\n",
    "    \n",
    "    # Initialize lists to store joint coordinates across frames\n",
    "    joint_x = [[] for _ in range(num_joints)]\n",
    "    joint_y = [[] for _ in range(num_joints)]\n",
    "    \n",
    "    # Iterate through each frame and each joint to extract coordinates\n",
    "    for frame_idx in range(num_frames):\n",
    "        frame = all_frame_data[frame_idx]\n",
    "        for joint_idx in range(num_joints):\n",
    "            landmark = frame[joint_idx]\n",
    "            joint_x[joint_idx].append(landmark.get_x())  # Replace with actual method to get x coordinate\n",
    "            joint_y[joint_idx].append(landmark.get_y())  # Replace with actual method to get y coordinate\n",
    "    \n",
    "    # Convert lists to NumPy arrays for plotting convenience\n",
    "    joint_x = np.array(joint_x)\n",
    "    joint_y = np.array(joint_y)\n",
    "    \n",
    "    return joint_x, joint_y\n",
    "\n",
    "# Example usage\n",
    "joint_x, joint_y = extract_joint_data(normalized_data)\n",
    "\n",
    "# Plotting pair plot for all joints\n",
    "num_joints = joint_x.shape[0]\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(num_joints):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    sns.scatterplot(x=joint_x[i], y=joint_y[i], marker='o', s=10)\n",
    "    plt.title(f'Joint {i + 1}')\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
