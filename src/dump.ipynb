{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic boiler plate code don't run this tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "cap = cv2.VideoCapture('/Users/suryanshpatel/Projects/Pose_detection/correct-squat-side-view.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Convert the image back to BGR for rendering\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw the pose annotation on the image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Pose Detection', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trash plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_joint_points(image, results):\n",
    "    joint_points = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            joint_points.append((landmark.x, landmark.y))\n",
    "    return joint_points\n",
    "\n",
    "def plot_joint_points(all_joint_points):\n",
    "    for i, points in enumerate(zip(*all_joint_points)):\n",
    "        xs, ys = zip(*points)\n",
    "        plt.figure()\n",
    "        plt.plot(xs, ys, marker='o', linestyle='-')\n",
    "        plt.title(f'Joint {i} Motion Over Time')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "all_joint_points = [] \n",
    "\n",
    "for frame in key_frames:\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    joint_points = capture_joint_points(frame, results)\n",
    "    all_joint_points.append(joint_points)\n",
    "\n",
    "# plot_joint_points(all_joint_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only displaying one frame may be vs code problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Function to animate the joint points\n",
    "def animate_joint_points(all_joint_points):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)  # Invert y-axis to match image coordinates\n",
    "\n",
    "    # Initialize plot\n",
    "    scat = ax.scatter([], [], c='blue')\n",
    "\n",
    "    def update(frame):\n",
    "        joint_points = all_joint_points[frame]\n",
    "        xs, ys = zip(*joint_points)\n",
    "        scat.set_offsets(np.c_[xs, ys])\n",
    "        ax.set_title(f'Frame {frame}')\n",
    "        return scat,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(all_joint_points), blit=True, repeat=False)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to animate joint points\n",
    "animate_joint_points(all_joint_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames coodinated scaled\n",
    "def plot_coordinates(coordinates, frame_number, save_folder):\n",
    "    plt.figure()\n",
    "    xs, ys = zip(*coordinates)\n",
    "    plt.plot(xs, ys, marker='o', linestyle='-')\n",
    "    plt.title(f'Joint Coordinates - Frame {frame_number}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.savefig(os.path.join(save_folder, f'coordinates_frame_{frame_number:03d}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view.mp4'\n",
    "output_folder = '/Users/suryanshpatel/Projects/Pose_detection/annotated_frames'\n",
    "coordinates_folder = '/Users/suryanshpatel/Projects/Pose_detection/coordinates'\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(coordinates_folder, exist_ok=True)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter for annotated video\n",
    "output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view-skeleton2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize variables for key frames\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Create a blank image (white background)\n",
    "    blank_image = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Draw the pose annotation on the blank image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Save annotated frame\n",
    "        annotated_frame_path = os.path.join(output_folder, f'frame_{len(key_frames):03d}.png')\n",
    "        cv2.imwrite(annotated_frame_path, cv2.cvtColor(blank_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Optionally plot and save coordinates\n",
    "        if results.pose_landmarks:\n",
    "            joint_coordinates = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]\n",
    "            plot_coordinates(joint_coordinates, frame_number=len(key_frames), save_folder=coordinates_folder)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(blank_image)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')\n",
    "print(f'Annotated frames saved in {output_folder}')\n",
    "print(f'Coordinate plots saved in {coordinates_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below code is to save skeleton video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect-squat-side-view.mp4'#'/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view.mp4'\n",
    "output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/correct-squat-side-view-skeleton3.mp4'\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 files\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Create a blank image (white background)\n",
    "    blank_image = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Draw the pose annotation on the blank image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(blank_image)\n",
    "\n",
    "    # Optionally display the image (comment out if not needed)\n",
    "    # cv2.imshow('Pose Detection', blank_image)\n",
    "    # if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "    #     break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO capture Video from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "systematic video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def initialize_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    return cap, frame_width, frame_height, fps\n",
    "\n",
    "def initialize_video_writer(output_video_path, frame_width, frame_height, fps):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    return out\n",
    "\n",
    "def is_key_frame(prev_frame, current_frame):\n",
    "    # Implement your logic to determine key frames\n",
    "    return False  # Placeholder for demo\n",
    "\n",
    "def annotate_frame(frame, results):\n",
    "    blank_image = np.ones_like(frame) * 255  # White background\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    return blank_image\n",
    "\n",
    "def save_frame(frame, output_folder, frame_number):\n",
    "    annotated_frame_path = os.path.join(output_folder, f'frame_{frame_number:03d}.png')\n",
    "    cv2.imwrite(annotated_frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def plot_coordinates(coordinates, frame_number, frame_width, frame_height, save_folder):\n",
    "    plt.figure()\n",
    "    scaled_coordinates = [(x * frame_width, y * frame_height) for x, y in coordinates]\n",
    "    xs, ys = zip(*scaled_coordinates)\n",
    "    plt.plot(xs, ys, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Joint Coordinates - Frame {frame_number}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.xlim(0, frame_width)\n",
    "    plt.ylim(frame_height, 0)  # Invert y-axis limits\n",
    "    plt.savefig(os.path.join(save_folder, f'coordinates_frame_{frame_number:03d}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def process_video(input_video_path, output_folder, coordinates_folder, output_video_path):\n",
    "    # Create output folders if they don't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(coordinates_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize video capture\n",
    "    cap, frame_width, frame_height, fps = initialize_video_capture(input_video_path)\n",
    "\n",
    "    # Initialize VideoWriter for annotated video\n",
    "    out = initialize_video_writer(output_video_path, frame_width, frame_height, fps)\n",
    "\n",
    "    # Initialize variables for key frames\n",
    "    key_frames = []\n",
    "    prev_frame = None\n",
    "    frame_number = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "            key_frames.append(frame)\n",
    "            prev_frame = gray_frame\n",
    "\n",
    "        # Process the image and detect the pose\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Annotate and save the frame\n",
    "        annotated_frame = annotate_frame(frame, results)\n",
    "        save_frame(annotated_frame, output_folder, frame_number)\n",
    "\n",
    "        # Plot and save coordinates\n",
    "        if results.pose_landmarks:\n",
    "            joint_coordinates = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]\n",
    "            plot_coordinates(joint_coordinates, frame_number, frame_width, frame_height, coordinates_folder)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    # cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "    print(f'Output video saved as {output_video_path}')\n",
    "    print(f'Annotated frames saved in {output_folder}')\n",
    "    print(f'Coordinate plots saved in {coordinates_folder}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect-squat-side-view.mp4'\n",
    "    output_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/annotated_frames/af5'\n",
    "    coordinates_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/plots/co5'\n",
    "    output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/incorrect_5.mp4'\n",
    "    process_video(input_video_path, output_folder, coordinates_folder, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to cut video on exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def is_key_frame(prev_frame, current_frame, threshold=30):\n",
    "    diff = cv2.absdiff(prev_frame, current_frame)\n",
    "    non_zero_count = np.count_nonzero(diff)\n",
    "    return non_zero_count > threshold\n",
    "\n",
    "def is_exercising(pose_landmarks, model):\n",
    "    # Convert pose landmarks to a format suitable for the model\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in pose_landmarks.landmark]).flatten()\n",
    "    landmarks = landmarks.reshape(1, -1)  # Reshape for the model\n",
    "    prediction = model.predict(landmarks)\n",
    "    return prediction[0] == 1  # Assuming the model returns 1 for exercise and 0 for non-exercise\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_frame = None\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the pre-trained model (replace with your actual model loading code)\n",
    "model = tf.keras.models.load_model('path_to_your_model.h5')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 30.0  # Assuming a frame rate of 30 FPS\n",
    "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "recording = False\n",
    "video_writer = None\n",
    "segment_count = 0\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "            results = pose.process(rgb_frame)\n",
    "            if results.pose_landmarks and is_exercising(results.pose_landmarks, model):\n",
    "                if not recording:\n",
    "                    # Start a new video segment\n",
    "                    recording = True\n",
    "                    segment_count += 1\n",
    "                    video_writer = cv2.VideoWriter(f'exercise_segment_{segment_count}.avi', fourcc, fps, frame_size)\n",
    "                \n",
    "                # Write the frame to the video file\n",
    "                video_writer.write(frame)\n",
    "            else:\n",
    "                if recording:\n",
    "                    # Stop recording the current video segment\n",
    "                    recording = False\n",
    "                    video_writer.release()\n",
    "                    video_writer = None\n",
    "\n",
    "            prev_frame = gray_frame\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Key Frame Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video writer if recording\n",
    "if recording and video_writer is not None:\n",
    "    video_writer.release()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to detect multiple people and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# Adjust indexing to handle both cases where net.getUnconnectedOutLayers() returns a 1D or 2D array\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Function to normalize joint points within bounding box\n",
    "def normalize_joint_points_within_box(joint_points, box):\n",
    "    x_min, y_min, width, height = box\n",
    "    normalized_points = []\n",
    "    for (x, y) in joint_points:\n",
    "        norm_x = (x - x_min) / width\n",
    "        norm_y = (y - y_min) / height\n",
    "        normalized_points.append((norm_x, norm_y))\n",
    "    return normalized_points\n",
    "\n",
    "# Function to compare joint points\n",
    "def compare_joint_points(user_points, professional_points):\n",
    "    differences = []\n",
    "    for user_point, professional_point in zip(user_points, professional_points):\n",
    "        diff = np.linalg.norm(np.array(user_point) - np.array(professional_point))\n",
    "        differences.append(diff)\n",
    "    return differences\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "all_joint_points = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Get bounding box for the person\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > 0.5:  # Class 0 is for person\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = max(0, int(center_x - w / 2))\n",
    "                y = max(0, int(center_y - h / 2))\n",
    "                w = min(w, width - x)\n",
    "                h = min(h, height - y)\n",
    "\n",
    "                # Extract the region within the bounding box\n",
    "                roi = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                if roi.size == 0:  # Check if the ROI is empty\n",
    "                    continue\n",
    "\n",
    "                # Process the region with MediaPipe for pose estimation\n",
    "                rgb_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(rgb_frame)\n",
    "\n",
    "                # Capture joint points\n",
    "                joint_points = []\n",
    "                if results.pose_landmarks:\n",
    "                    for landmark in results.pose_landmarks.landmark:\n",
    "                        joint_points.append((landmark.x * w + x, landmark.y * h + y))\n",
    "\n",
    "                # Normalize joint points within bounding box\n",
    "                normalized_points = normalize_joint_points_within_box(joint_points, (x, y, w, h))\n",
    "                all_joint_points.append(normalized_points)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw the pose annotation on the image\n",
    "                if results.pose_landmarks:\n",
    "                    mp_drawing.draw_landmarks(roi, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Optionally display the image\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points)\n",
    "\n",
    "# Load professional's joint points (normalized) for comparison\n",
    "professional_joint_points = np.load('professional_joint_points.npy')\n",
    "\n",
    "# Compare user's joint points with professional's\n",
    "differences = []\n",
    "for user_points in all_joint_points:\n",
    "    differences.append(compare_joint_points(user_points, professional_joint_points))\n",
    "\n",
    "# Convert differences to NumPy array for easier processing\n",
    "differences = np.array(differences)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, joint_diff in enumerate(differences.T):\n",
    "    plt.plot(joint_diff, label=f'Joint {i+1}')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Difference')\n",
    "plt.title('Joint Differences between User and Professional')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working code with yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "all_joint_points = []\n",
    "best_box_dimensions = []\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 files\n",
    "# output = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Track the best detection\n",
    "    best_confidence = 0\n",
    "    best_box = None\n",
    "\n",
    "    # Get bounding box for the person with the highest confidence\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > best_confidence:  # Class 0 is for person\n",
    "                best_confidence = confidence\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = max(0, int(center_x - w / 2))\n",
    "                y = max(0, int(center_y - h / 2))\n",
    "                w = min(w, width - x)\n",
    "                h = min(h, height - y)\n",
    "                best_box = (x, y, w, h)\n",
    "\n",
    "    if best_box:\n",
    "        x, y, w, h = best_box\n",
    "\n",
    "        # Extract the region within the bounding box\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        if roi.size == 0:  # Check if the ROI is empty\n",
    "            continue\n",
    "\n",
    "        # Process the region with MediaPipe for pose estimation\n",
    "        rgb_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Capture joint points\n",
    "        joint_points = []\n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                joint_points.append((landmark.x * w + x, landmark.y * h + y))\n",
    "\n",
    "        \n",
    "        #Surya - don't want to normalize now store as it is as we are storing box dimentions as well\n",
    "        #  Normalize joint points within bounding box\n",
    "        # normalized_points = normalize_joint_points_within_box(joint_points, (x, y, w, h))\n",
    "        # all_joint_points.append(normalized_points)\n",
    "        \n",
    "        \n",
    "        all_joint_points.append(joint_points)\n",
    "        best_box_dimensions.append((x, y, w, h))\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the pose annotation on the image\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(roi, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    # output.write(frame)\n",
    "\n",
    "    # Optionally display the image\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "# output.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points , dtype=object)\n",
    "best_box_dimensions = np.array(best_box_dimensions)\n",
    "\n",
    "print(all_joint_points.shape , best_box_dimensions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plots every joint points not adding arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example function to extract data from all_frame_data\n",
    "def extract_joint_data(all_frame_data):\n",
    "    num_frames = all_frame_data.shape[0]\n",
    "    num_joints = len(all_frame_data[0])  # Assuming all frames have the same number of joints\n",
    "    \n",
    "    # Initialize lists to store joint coordinates across frames\n",
    "    joint_x = [[] for _ in range(num_joints)]\n",
    "    joint_y = [[] for _ in range(num_joints)]\n",
    "    \n",
    "    # Iterate through each frame and each joint to extract coordinates\n",
    "    for frame_idx in range(num_frames):\n",
    "        frame = all_frame_data[frame_idx]\n",
    "        for joint_idx in range(num_joints):\n",
    "            landmark = frame[joint_idx]\n",
    "            joint_x[joint_idx].append(landmark.get_x())  # Replace with actual method to get x coordinate\n",
    "            joint_y[joint_idx].append(landmark.get_y())  # Replace with actual method to get y coordinate\n",
    "    \n",
    "    # Convert lists to NumPy arrays for plotting convenience\n",
    "    joint_x = np.array(joint_x)\n",
    "    joint_y = np.array(joint_y)\n",
    "    \n",
    "    return joint_x, joint_y\n",
    "\n",
    "# Example usage\n",
    "joint_x, joint_y = extract_joint_data(normalized_data)\n",
    "\n",
    "# Plotting pair plot for all joints\n",
    "num_joints = joint_x.shape[0]\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(num_joints):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    sns.scatterplot(x=joint_x[i], y=joint_y[i], marker='o', s=10)\n",
    "    plt.title(f'Joint {i + 1}')\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames animation for 2 exercises not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to prepare data for animation\n",
    "def prepare_data_for_animation(normalized_data):\n",
    "    num_frames = normalized_data.shape[0]\n",
    "    num_joints = len(normalized_data[0])  # Assuming all frames have the same number of joints\n",
    "    \n",
    "    # Initialize lists to store normalized joint coordinates across frames\n",
    "    joint_x_normalized = [[] for _ in range(num_joints)]\n",
    "    joint_y_normalized = [[] for _ in range(num_joints)]\n",
    "    \n",
    "    # Iterate through each frame and each joint to extract normalized coordinates\n",
    "    for frame_idx in range(num_frames):\n",
    "        frame = normalized_data[frame_idx]\n",
    "        for joint_idx in range(num_joints):\n",
    "            landmark = frame[joint_idx]\n",
    "            norm_x = landmark.get_x()  # Replace with actual method to get normalized x coordinate\n",
    "            norm_y = landmark.get_y()  # Replace with actual method to get normalized y coordinate\n",
    "            joint_x_normalized[joint_idx].append(norm_x)\n",
    "            joint_y_normalized[joint_idx].append(norm_y)\n",
    "    \n",
    "    # Convert lists to NumPy arrays for plotting convenience\n",
    "    joint_x_normalized = np.array(joint_x_normalized)\n",
    "    joint_y_normalized = np.array(joint_y_normalized)\n",
    "    \n",
    "    return joint_x_normalized, joint_y_normalized\n",
    "\n",
    "# Example function to update frames in animation\n",
    "def update_frame(frame_idx, data, scatters):\n",
    "    for i in range(len(scatters)):\n",
    "        scatters[i].set_offsets(np.c_[data[0][i][frame_idx], data[1][i][frame_idx]])\n",
    "    return scatters\n",
    "\n",
    "# Example usage with normalized data\n",
    "joint_x_normalized, joint_y_normalized = prepare_data_for_animation(normalized_data)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Initialize scatter plot with empty data\n",
    "scatters = [ax.scatter([], [], s=10) for _ in range(len(joint_x_normalized))]\n",
    "\n",
    "# Set axis limits and labels\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Normalized X coordinate')\n",
    "ax.set_ylabel('Normalized Y coordinate')\n",
    "\n",
    "# Animation function\n",
    "ani = animation.FuncAnimation(fig, update_frame, frames=len(joint_x_normalized[0]), fargs=((joint_x_normalized, joint_y_normalized), scatters), interval=50, blit=True)\n",
    "\n",
    "plt.title('Movement of Joints over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph will be upside down because matplot lib starts drawing graph form bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(graph, title=\"Graph Visualization\"):\n",
    "    pos = nx.get_node_attributes(graph, 'pos')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=300, node_color='skyblue', edge_color='gray', font_size=10, font_color='black')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example function to visualize multiple graphs (e.g., from a sequence of frames)\n",
    "def visualize_graph_sequence(graphs, start=0, end=10):\n",
    "    for i in range(start, end):\n",
    "        draw_graph(graphs[i], title=f\"Graph at Frame {i}\")\n",
    "\n",
    "# Example usage to visualize a single graph\n",
    "draw_graph(graphs_video1[0], title=\"Graph at Frame 0 (Video 1)\")\n",
    "\n",
    "# Example usage to visualize a sequence of graphs\n",
    "# visualize_graph_sequence(graphs_video1, start=0, end=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to classify exercise    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class_labels = {\n",
    "    0: 'jumping_jacks',\n",
    "    1: 'pushups',\n",
    "    2: 'squats',\n",
    "    # Add more classes as needed\n",
    "}\n",
    "\n",
    "def postprocess(outputs, threshold=0.5, window_size=5):\n",
    "    # Step 1: Apply softmax to get probabilities\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Step 2: Thresholding (if needed)\n",
    "    if threshold is not None:\n",
    "        probabilities = probabilities > threshold\n",
    "    \n",
    "    # Step 3: Smoothing (if needed)\n",
    "    if window_size is not None and window_size > 1:\n",
    "        smoothed_predictions = []\n",
    "        for i in range(len(probabilities) - window_size + 1):\n",
    "            window = probabilities[i:i+window_size]\n",
    "            majority_vote = torch.mode(window, dim=0).values\n",
    "            smoothed_predictions.append(majority_vote)\n",
    "        probabilities = torch.tensor(smoothed_predictions)\n",
    "    \n",
    "    # Map to class labels\n",
    "    _, predicted_indices = torch.max(probabilities, 1)\n",
    "    predicted_labels = [class_labels[idx.item()] for idx in predicted_indices]\n",
    "    \n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW_8x8_R50.pyth\" to /Users/suryanshpatel/.cache/torch/hub/checkpoints/SLOW_8x8_R50.pyth\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n",
      "\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1344\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1345\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n",
      "\u001b[1;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m-> 1382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n",
      "\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n",
      "\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n",
      "\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n",
      "\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   1094\u001b[0m \n",
      "\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n",
      "\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1477\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1475\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "\u001b[0;32m-> 1477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1478\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n",
      "\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n",
      "\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n",
      "\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n",
      "\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n",
      "\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m slow_r50\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load a pre-trained SlowFast model from PyTorchVideo\u001b[39;00m\n",
      "\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mslow_r50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare your data as needed\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# You will need to preprocess your keypoints and frames appropriately\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Projects/Pose_detection/.venv/lib/python3.12/site-packages/pytorchvideo/models/hub/resnet.py:62\u001b[0m, in \u001b[0;36mslow_r50\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslow_r50\u001b[39m(\n",
      "\u001b[1;32m     42\u001b[0m     pretrained: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n",
      "\u001b[1;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n",
      "\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Slow R50 model architecture [1] with pretrained weights based on 8x8 setting\u001b[39;00m\n",
      "\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    on the Kinetics dataset. Model with pretrained weights has top1 accuracy of 74.58.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    without pretrained weights.\u001b[39;00m\n",
      "\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslow_r50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstem_conv_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_pool_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Projects/Pose_detection/.venv/lib/python3.12/site-packages/pytorchvideo/models/hub/resnet.py:33\u001b[0m, in \u001b[0;36m_resnet\u001b[0;34m(pretrained, progress, checkpoint_path, model_builder, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m model_builder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n",
      "\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# All models are loaded onto CPU by default\u001b[39;00m\n",
      "\u001b[0;32m---> 33\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     36\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\n",
      "File \u001b[0;32m~/Projects/Pose_detection/.venv/lib/python3.12/site-packages/torch/hub.py:760\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n",
      "\u001b[1;32m    758\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n",
      "\u001b[1;32m    759\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m--> 760\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n",
      "\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\n",
      "File \u001b[0;32m~/Projects/Pose_detection/.venv/lib/python3.12/site-packages/torch/hub.py:622\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n",
      "\u001b[1;32m    620\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    621\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[0;32m--> 622\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    623\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n",
      "\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n",
      "\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n",
      "\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n",
      "\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n",
      "\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n",
      "\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n",
      "\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n",
      "\u001b[1;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n",
      "\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n",
      "\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n",
      "\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n",
      "\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n",
      "\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n",
      "\u001b[1;32m   1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n",
      "\u001b[1;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "\u001b[1;32m   1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n",
      "\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorchvideo.models.hub import slow_r50\n",
    "\n",
    "# Load a pre-trained SlowFast model from PyTorchVideo\n",
    "model = slow_r50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Prepare your data as needed\n",
    "# You will need to preprocess your keypoints and frames appropriately\n",
    "\n",
    "def classify_exercise(frames):\n",
    "    # Preprocess frames to the required input format for the model\n",
    "    # Placeholder preprocessing\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "    \n",
    "    # Post-process outputs to get class predictions\n",
    "    predicted_classes = postprocess(outputs)\n",
    "    \n",
    "    return predicted_classes\n",
    "\n",
    "\n",
    "print(classify_exercise(coordinates_video1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
