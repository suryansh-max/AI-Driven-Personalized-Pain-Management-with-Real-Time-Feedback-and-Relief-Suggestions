{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0IEYtohPpge"
      },
      "outputs": [],
      "source": [
        "# Data visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "# import keras_tuner as kt\n",
        "\n",
        "# Train-Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Classification Report\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S8y3JalPpgf"
      },
      "source": [
        "## 1. Set up important landmarks and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adXGogRrPpgg"
      },
      "outputs": [],
      "source": [
        "# Determine important landmarks for lunge\n",
        "IMPORTANT_LMS = [\n",
        "    \"NOSE\",\n",
        "    \"LEFT_SHOULDER\",\n",
        "    \"RIGHT_SHOULDER\",\n",
        "    \"LEFT_HIP\",\n",
        "    \"RIGHT_HIP\",\n",
        "    \"LEFT_KNEE\",\n",
        "    \"RIGHT_KNEE\",\n",
        "    \"LEFT_ANKLE\",\n",
        "    \"RIGHT_ANKLE\",\n",
        "    \"LEFT_HEEL\",\n",
        "    \"RIGHT_HEEL\",\n",
        "    \"LEFT_FOOT_INDEX\",\n",
        "    \"RIGHT_FOOT_INDEX\",\n",
        "]\n",
        "\n",
        "# Generate all columns of the data frame\n",
        "\n",
        "HEADERS = [\"label\"] # Label column\n",
        "\n",
        "for lm in IMPORTANT_LMS:\n",
        "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]\n",
        "\n",
        "TRAIN_SET_PATH  = \"/content/err.train.csv\"\n",
        "TEST_SET_PATH  = \"/content/err.test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGouJ9MqPpgg"
      },
      "outputs": [],
      "source": [
        "def describe_dataset(dataset_path: str):\n",
        "    '''\n",
        "    Describe dataset\n",
        "    '''\n",
        "\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    print(f\"Headers: {list(data.columns.values)}\")\n",
        "    print(f'Number of rows: {data.shape[0]} \\nNumber of columns: {data.shape[1]}\\n')\n",
        "    print(f\"Labels: \\n{data['label'].value_counts()}\\n\")\n",
        "    print(f\"Missing values: {data.isnull().values.any()}\\n\")\n",
        "\n",
        "    duplicate = data[data.duplicated()]\n",
        "    print(f\"Duplicate Rows : {len(duplicate.sum(axis=1))}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Remove duplicate rows (optional)\n",
        "def remove_duplicate_rows(dataset_path: str):\n",
        "    '''\n",
        "    Remove duplicated data from the dataset then save it to another files\n",
        "    '''\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df.drop_duplicates(keep=\"first\", inplace=True)\n",
        "    df.to_csv(f\"cleaned_dataset.csv\", sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def round_up_metric_results(results) -> list:\n",
        "    '''Round up metrics results such as precision score, recall score, ...'''\n",
        "    return list(map(lambda el: round(el, 3), results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXpOTKQRPpgg"
      },
      "source": [
        "## 2. Describe and process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_I2fWN_Ppgg",
        "outputId": "3f9c671f-df17-4b70-81d3-164ff66aca00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers: ['label', 'nose_x', 'nose_y', 'nose_z', 'nose_v', 'left_shoulder_x', 'left_shoulder_y', 'left_shoulder_z', 'left_shoulder_v', 'right_shoulder_x', 'right_shoulder_y', 'right_shoulder_z', 'right_shoulder_v', 'left_hip_x', 'left_hip_y', 'left_hip_z', 'left_hip_v', 'right_hip_x', 'right_hip_y', 'right_hip_z', 'right_hip_v', 'left_knee_x', 'left_knee_y', 'left_knee_z', 'left_knee_v', 'right_knee_x', 'right_knee_y', 'right_knee_z', 'right_knee_v', 'left_ankle_x', 'left_ankle_y', 'left_ankle_z', 'left_ankle_v', 'right_ankle_x', 'right_ankle_y', 'right_ankle_z', 'right_ankle_v', 'left_heel_x', 'left_heel_y', 'left_heel_z', 'left_heel_v', 'right_heel_x', 'right_heel_y', 'right_heel_z', 'right_heel_v', 'left_foot_index_x', 'left_foot_index_y', 'left_foot_index_z', 'left_foot_index_v', 'right_foot_index_x', 'right_foot_index_y', 'right_foot_index_z', 'right_foot_index_v']\n",
            "Number of rows: 17907 \n",
            "Number of columns: 53\n",
            "\n",
            "Labels: \n",
            "label\n",
            "L    9114\n",
            "C    8793\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing values: False\n",
            "\n",
            "Duplicate Rows : 0\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "df = describe_dataset(TRAIN_SET_PATH)\n",
        "\n",
        "# Categorizing label\n",
        "df.loc[df[\"label\"] == \"L\", \"label\"] = 0\n",
        "df.loc[df[\"label\"] == \"C\", \"label\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbtxaoSTPpgh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "# Standard Scaling of features\n",
        "with open(\"/content/input_scaler.pkl\", \"rb\") as f2:\n",
        "    input_scaler = pickle.load(f2)\n",
        "\n",
        "x = df.drop(\"label\", axis = 1)\n",
        "x = pd.DataFrame(input_scaler.transform(x))\n",
        "\n",
        "y = df[\"label\"]\n",
        "\n",
        "# # Converting prediction to categorical\n",
        "y_cat = to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ApXbT6uPpgh"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y_cat, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhK2IjcJPpgh"
      },
      "source": [
        "## 3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXCQ11FkPpgh"
      },
      "source": [
        "### 3.1. Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQdMZn96Ppgh"
      },
      "outputs": [],
      "source": [
        "stop_early = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# Final Results\n",
        "final_models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi6ut8tlPpgi"
      },
      "outputs": [],
      "source": [
        "def describe_model(model):\n",
        "    '''\n",
        "    Describe Model architecture\n",
        "    '''\n",
        "    print(f\"Describe models architecture\")\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        number_of_units = layer.units if hasattr(layer, 'units') else 0\n",
        "\n",
        "        if hasattr(layer, \"activation\"):\n",
        "            print(f\"Layer-{i + 1}: {number_of_units} units, func: \", layer.activation)\n",
        "        else:\n",
        "            print(f\"Layer-{i + 1}: {number_of_units} units, func: None\")\n",
        "\n",
        "\n",
        "def get_best_model(tuner):\n",
        "    '''\n",
        "    Describe and return the best model found from keras tuner\n",
        "    '''\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    describe_model(best_model)\n",
        "\n",
        "    for h_param in [\"learning_rate\"]:\n",
        "        print(f\"{h_param}: {tuner.get_best_hyperparameters()[0].get(h_param)}\")\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKEWYgECPpgi"
      },
      "source": [
        "### 3.2. Model with 3 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEvM8FNBPpgi"
      },
      "outputs": [],
      "source": [
        "def model_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_dim = 52, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install keras-tuner\n"
      ],
      "metadata": {
        "id": "IIV60CtXRdzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmveEEbfPpgi"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "tuner = kt.Hyperband(\n",
        "    model_builder,\n",
        "    objective='accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f46ez6oKPpgi"
      },
      "outputs": [],
      "source": [
        "# tuner.search(x_train, y_train, epochs=10, callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y4i4PJbPpgi",
        "outputId": "beace26a-a64d-44cf-d860-c98ab4b70438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-2: 352 units, func:  <function tanh at 0x79907ff84040>\n",
            "Layer-3: 2 units, func:  <function softmax at 0x79907ffb7be0>\n",
            "learning_rate: 0.001\n",
            "Epoch 1/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.0834 - val_accuracy: 0.9994 - val_loss: 0.0030\n",
            "Epoch 2/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0108 - val_accuracy: 0.9997 - val_loss: 0.0015\n",
            "Epoch 3/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 0.9997 - val_loss: 0.0016\n",
            "Epoch 4/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0014\n",
            "Epoch 5/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0050 - val_accuracy: 0.9980 - val_loss: 0.0049\n",
            "Epoch 6/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 0.0014 - val_accuracy: 0.9994 - val_loss: 0.0013\n",
            "Epoch 7/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9997 - val_loss: 0.0014\n",
            "Epoch 8/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 0.9992 - val_loss: 0.0013\n",
            "Epoch 9/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 0.9997 - val_loss: 3.8396e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.2999e-05 - val_accuracy: 0.9989 - val_loss: 0.0045\n",
            "Epoch 11/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 0.9992 - val_loss: 9.2640e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6032e-04 - val_accuracy: 0.9997 - val_loss: 6.4782e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 8.8507e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.9992 - val_loss: 0.0020\n",
            "Epoch 15/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 8.2972e-04 - val_accuracy: 1.0000 - val_loss: 1.6496e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7990688e7670>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model = get_best_model(tuner)\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zifEPTqNPpgj"
      },
      "outputs": [],
      "source": [
        "final_models[\"3_layers\"] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HXhurO7Ppgj"
      },
      "source": [
        "### 3.3. Model with 5 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgzSRszfPpgj"
      },
      "outputs": [],
      "source": [
        "def model_builder_5(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(52, input_dim = 52, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOf0-G_bPpgj",
        "outputId": "fbd521c1-66ba-4059-bc8f-3f7062f66617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from keras_tuner_dir/keras_tuner_demo_2/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "tuner = kt.Hyperband(\n",
        "    model_builder_5,\n",
        "    objective='accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_2'\n",
        ")\n",
        "# tuner.search(x_train, y_train, epochs=10, callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp1jZ0gIPpgk",
        "outputId": "0f091869-c19c-4191-8890-b7c2d8965564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-2: 160 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-3: 288 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-4: 384 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-5: 2 units, func:  <function softmax at 0x79907ffb7be0>\n",
            "learning_rate: 0.0001\n",
            "Epoch 1/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9519 - loss: 0.1770 - val_accuracy: 0.9986 - val_loss: 0.0072\n",
            "Epoch 2/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0073 - val_accuracy: 0.9992 - val_loss: 0.0050\n",
            "Epoch 3/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9994 - val_loss: 0.0025\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799061e4f640>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model_5 = get_best_model(tuner)\n",
        "model_5.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdhxOztkPpgk"
      },
      "outputs": [],
      "source": [
        "final_models[\"5_layers\"] = model_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev8EC0FIPpgk"
      },
      "source": [
        "### 3.4. Model with 7 layers along with Dropout layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nDylQJfPpgk"
      },
      "outputs": [],
      "source": [
        "def model_builder_dropout_5(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(52, input_dim = 52, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYm9WSCqPpgk"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(\n",
        "    model_builder_dropout_5,\n",
        "    objective='accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_3'\n",
        ")\n",
        "# tuner.search(x_train, y_train, epochs=10, callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VezAa2jaPpgk",
        "outputId": "1d32795b-115d-4e07-d47f-d00e20ae8c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x7990de7bf1c0>\n",
            "Layer-2: 224 units, func:  <function tanh at 0x79907ff84040>\n",
            "Layer-3: 0 units, func: None\n",
            "Layer-4: 288 units, func:  <function tanh at 0x79907ff84040>\n",
            "Layer-5: 0 units, func: None\n",
            "Layer-6: 128 units, func:  <function tanh at 0x79907ff84040>\n",
            "Layer-7: 2 units, func:  <function softmax at 0x79907ffb7be0>\n",
            "learning_rate: 0.01\n",
            "Epoch 1/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9227 - loss: 0.2356 - val_accuracy: 0.9978 - val_loss: 0.0082\n",
            "Epoch 2/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0291 - val_accuracy: 0.9983 - val_loss: 0.0101\n",
            "Epoch 3/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0233 - val_accuracy: 0.9886 - val_loss: 0.0444\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7990699f0bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model_5_with_dropout = get_best_model(tuner)\n",
        "model_5_with_dropout.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9aKyNL0Ppgk"
      },
      "outputs": [],
      "source": [
        "final_models[\"7_layers_with_dropout\"] = model_5_with_dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmTY-fQDPpgl"
      },
      "source": [
        "### 3.5. Model with 7 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N4W0tHHPpgl"
      },
      "outputs": [],
      "source": [
        "def model_builder_7(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(52, input_dim = 52, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_4 = hp.Int('layer_4', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_5 = hp.Int('layer_5', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_4, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_5, activation=hp_activation))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "qyXNnlUyy7jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLOkScB4Ppgl"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "tuner = kt.Hyperband(\n",
        "    model_builder_7,\n",
        "    objective='accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_4'\n",
        ")\n",
        "tuner.search(x_train, y_train, epochs=10, callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doLEaI5QPpgl",
        "outputId": "51ae12aa-cb40-47c1-f581-739eee68357c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x784721864670>\n",
            "Layer-2: 288 units, func:  <function tanh at 0x7847211e3250>\n",
            "Layer-3: 256 units, func:  <function tanh at 0x7847211e3250>\n",
            "Layer-4: 192 units, func:  <function tanh at 0x7847211e3250>\n",
            "Layer-5: 96 units, func:  <function tanh at 0x7847211e3250>\n",
            "Layer-6: 480 units, func:  <function tanh at 0x7847211e3250>\n",
            "Layer-7: 2 units, func:  <function softmax at 0x7847211e2e60>\n",
            "learning_rate: 0.0001\n",
            "Epoch 1/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9552 - loss: 0.1162 - val_accuracy: 0.9992 - val_loss: 0.0034\n",
            "Epoch 2/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9966 - loss: 0.0098 - val_accuracy: 0.9992 - val_loss: 0.0018\n",
            "Epoch 3/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 0.9992 - val_loss: 0.0016\n",
            "Epoch 4/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9985 - loss: 0.0040 - val_accuracy: 0.9983 - val_loss: 0.0086\n",
            "Epoch 5/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 5.1199e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0015 - val_accuracy: 0.9986 - val_loss: 0.0035\n",
            "Epoch 7/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.2278e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9998 - loss: 7.9695e-04 - val_accuracy: 0.9735 - val_loss: 0.0707\n",
            "Epoch 9/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0031 - val_accuracy: 0.9997 - val_loss: 6.8342e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 8.1561e-04 - val_accuracy: 0.9958 - val_loss: 0.0205\n",
            "Epoch 11/100\n",
            "\u001b[1m1433/1433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.2908e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7847028bb6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_7 = get_best_model(tuner)\n",
        "model_7.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7.save(\"/content/my_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgjfz2eX0Lf5",
        "outputId": "2efd836e-4753-4c7a-c218-66e03511914b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDfQ2rlkPpgm"
      },
      "outputs": [],
      "source": [
        "final_models[\"7_layers\"] = model_7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y0-0l_vPpgm"
      },
      "source": [
        "### 3.6. Final Models Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt3zu9rdPpgm",
        "outputId": "640d22fc-08f1-4017-a756-49b79816db3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7_layers_with_dropout: Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-2: 288 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-3: 0 units, func: None\n",
            "Layer-4: 224 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-5: 0 units, func: None\n",
            "Layer-6: 320 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-7: 2 units, func:  <function softmax at 0x15db71160>\n",
            "\n",
            "5_layers: Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-2: 480 units, func:  <function tanh at 0x15db71ee0>\n",
            "Layer-3: 480 units, func:  <function tanh at 0x15db71ee0>\n",
            "Layer-4: 192 units, func:  <function tanh at 0x15db71ee0>\n",
            "Layer-5: 2 units, func:  <function softmax at 0x15db71160>\n",
            "\n",
            "3_layers: Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-2: 192 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-3: 2 units, func:  <function softmax at 0x15db71160>\n",
            "\n",
            "7_layers: Describe models architecture\n",
            "Layer-1: 52 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-2: 32 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-3: 416 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-4: 192 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-5: 224 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-6: 416 units, func:  <function relu at 0x15db71b80>\n",
            "Layer-7: 2 units, func:  <function softmax at 0x15db71160>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, model in final_models.items():\n",
        "    print(f\"{name}: \", end=\"\")\n",
        "    describe_model(model)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3xx-GBaPpgm"
      },
      "source": [
        "## 4. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIciCJIlPpgm"
      },
      "source": [
        "### 4.1. Train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrPlau07Ppgm",
        "outputId": "d7489a91-5746-454f-e0ad-32e7dd635b83"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 score</th>\n",
              "      <th>Confusion Matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7_layers_with_dropout</td>\n",
              "      <td>[1.0, 0.999]</td>\n",
              "      <td>[0.999, 1.0]</td>\n",
              "      <td>[1.0, 1.0]</td>\n",
              "      <td>[[1805, 1], [0, 1776]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3_layers</td>\n",
              "      <td>[1.0, 0.999]</td>\n",
              "      <td>[0.999, 1.0]</td>\n",
              "      <td>[1.0, 1.0]</td>\n",
              "      <td>[[1805, 1], [0, 1776]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7_layers</td>\n",
              "      <td>[1.0, 1.0]</td>\n",
              "      <td>[1.0, 1.0]</td>\n",
              "      <td>[1.0, 1.0]</td>\n",
              "      <td>[[1806, 0], [0, 1776]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5_layers</td>\n",
              "      <td>[1.0, 0.994]</td>\n",
              "      <td>[0.994, 1.0]</td>\n",
              "      <td>[0.997, 0.997]</td>\n",
              "      <td>[[1795, 11], [0, 1776]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Model Precision Score  Recall Score        F1 score  \\\n",
              "0  7_layers_with_dropout    [1.0, 0.999]  [0.999, 1.0]      [1.0, 1.0]   \n",
              "1               3_layers    [1.0, 0.999]  [0.999, 1.0]      [1.0, 1.0]   \n",
              "2               7_layers      [1.0, 1.0]    [1.0, 1.0]      [1.0, 1.0]   \n",
              "3               5_layers    [1.0, 0.994]  [0.994, 1.0]  [0.997, 0.997]   \n",
              "\n",
              "          Confusion Matrix  \n",
              "0   [[1805, 1], [0, 1776]]  \n",
              "1   [[1805, 1], [0, 1776]]  \n",
              "2   [[1806, 0], [0, 1776]]  \n",
              "3  [[1795, 11], [0, 1776]]  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_results = []\n",
        "\n",
        "for name, model in final_models.items():\n",
        "    # Evaluate model\n",
        "    predict_x = model.predict(x_test, verbose=False)\n",
        "    y_pred_class = np.argmax(predict_x, axis=1)\n",
        "    y_test_class = np.argmax(y_test, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test_class, y_pred_class, labels=[0, 1])\n",
        "    (p_score, r_score, f_score, _) = precision_recall_fscore_support(y_test_class, y_pred_class, labels=[0, 1])\n",
        "\n",
        "    train_set_results.append(( name, round_up_metric_results(p_score), round_up_metric_results(r_score), round_up_metric_results(f_score), cm ))\n",
        "\n",
        "train_set_results.sort(key=lambda k: sum(k[3]), reverse=True)\n",
        "pd.DataFrame(train_set_results, columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7s0uHzPpgm"
      },
      "source": [
        "### 4.2. Test set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUXfF9iIPpgz"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(TEST_SET_PATH)\n",
        "\n",
        "# Categorizing label\n",
        "test_df.loc[test_df[\"label\"] == \"L\", \"label\"] = 0\n",
        "test_df.loc[test_df[\"label\"] == \"C\", \"label\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLGRWyJiPpgz"
      },
      "outputs": [],
      "source": [
        "# Standard Scaling of features\n",
        "test_x = test_df.drop(\"label\", axis = 1)\n",
        "test_x = pd.DataFrame(input_scaler.transform(test_x))\n",
        "\n",
        "test_y = test_df[\"label\"]\n",
        "\n",
        "# # Converting prediction to categorical\n",
        "test_y_cat = to_categorical(test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sofvgUlxPpgz",
        "outputId": "48307ad8-5ebe-45b4-b25b-08a0d66743b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-25 15:46:42.537983: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-11-25 15:46:42.694947: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-11-25 15:46:42.853052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-11-25 15:46:42.974234: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 score</th>\n",
              "      <th>Confusion Matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3_layers</td>\n",
              "      <td>[0.998, 0.873]</td>\n",
              "      <td>[0.859, 0.998]</td>\n",
              "      <td>[0.923, 0.932]</td>\n",
              "      <td>[[482, 79], [1, 545]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7_layers_with_dropout</td>\n",
              "      <td>[0.995, 0.786]</td>\n",
              "      <td>[0.736, 0.996]</td>\n",
              "      <td>[0.846, 0.879]</td>\n",
              "      <td>[[413, 148], [2, 544]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5_layers</td>\n",
              "      <td>[0.963, 0.755]</td>\n",
              "      <td>[0.693, 0.973]</td>\n",
              "      <td>[0.806, 0.85]</td>\n",
              "      <td>[[389, 172], [15, 531]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7_layers</td>\n",
              "      <td>[0.984, 0.687]</td>\n",
              "      <td>[0.561, 0.991]</td>\n",
              "      <td>[0.715, 0.812]</td>\n",
              "      <td>[[315, 246], [5, 541]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Model Precision Score    Recall Score        F1 score  \\\n",
              "0               3_layers  [0.998, 0.873]  [0.859, 0.998]  [0.923, 0.932]   \n",
              "1  7_layers_with_dropout  [0.995, 0.786]  [0.736, 0.996]  [0.846, 0.879]   \n",
              "2               5_layers  [0.963, 0.755]  [0.693, 0.973]   [0.806, 0.85]   \n",
              "3               7_layers  [0.984, 0.687]  [0.561, 0.991]  [0.715, 0.812]   \n",
              "\n",
              "          Confusion Matrix  \n",
              "0    [[482, 79], [1, 545]]  \n",
              "1   [[413, 148], [2, 544]]  \n",
              "2  [[389, 172], [15, 531]]  \n",
              "3   [[315, 246], [5, 541]]  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_results = []\n",
        "\n",
        "for name, model in final_models.items():\n",
        "    # Evaluate model\n",
        "    predict_x = model.predict(test_x, verbose=False)\n",
        "    y_pred_class = np.argmax(predict_x, axis=1)\n",
        "    y_test_class = np.argmax(test_y_cat, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test_class, y_pred_class, labels=[0, 1])\n",
        "    (p_score, r_score, f_score, _) = precision_recall_fscore_support(y_test_class, y_pred_class, labels=[0, 1])\n",
        "\n",
        "    test_set_results.append(( name, round_up_metric_results(p_score), round_up_metric_results(r_score), round_up_metric_results(f_score), cm ))\n",
        "\n",
        "test_set_results.sort(key=lambda k: sum(k[3]), reverse=True)\n",
        "pd.DataFrame(test_set_results, columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oQpT9otPpgz"
      },
      "source": [
        "## 5. Dump Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiHglvJJPpgz",
        "outputId": "ca927030-5d61-4189-8fef-e00a15cce343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://4444bb6e-de2c-4bed-806c-db9d955522ad/assets\n"
          ]
        }
      ],
      "source": [
        "# Dump the best model to a pickle file\n",
        "with open(\"./model/dp/err_lunge_dp.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_models[\"3_layers\"], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "852-xRN8Ppgz",
        "outputId": "33e3ffd7-2404-4c1f-9774-2825d09924ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://0f5761b9-5d62-4bf6-8de5-34355293f17a/assets\n",
            "INFO:tensorflow:Assets written to: ram://ac2add31-fb1f-4baa-b925-718896fea315/assets\n",
            "INFO:tensorflow:Assets written to: ram://20b6d7c9-7ddf-42a1-aaba-c31e12762a10/assets\n",
            "INFO:tensorflow:Assets written to: ram://ee8c66ec-0ee3-4a5c-bc03-f12225018c47/assets\n"
          ]
        }
      ],
      "source": [
        "with open(\"./model/dp/all_models.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_models, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "DkON78ytxLoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msdJq0OVxpJp",
        "outputId": "32bc71f9-e520-4eaf-ddc4-84967b1f8d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.0)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVE2hylOPpgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d358496b-6878-4e38-dccd-1f386c1b6bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2024-09-11 02:03:08.700 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.813 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-09-11 02:03:08.817 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.820 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-11 02:03:08.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model from the current directory\n",
        "model = load_model(\"my_model.h5\")\n",
        "\n",
        "# Load scaler from the current directory\n",
        "with open(\"input_scaler.pkl\", \"rb\") as f:\n",
        "    input_scaler = pickle.load(f)\n",
        "\n",
        "# Define a function to make predictions on a DataFrame\n",
        "def make_prediction(df):\n",
        "    # Scale the input data\n",
        "    scaled_data = input_scaler.transform(df)\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(scaled_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    return predicted_classes\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"ML Model Deployment with Streamlit\")\n",
        "st.write(\"Upload your CSV file to get predictions\")\n",
        "\n",
        "# File uploader for CSV\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the CSV file\n",
        "    test_data = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # Display the uploaded CSV file\n",
        "    st.write(\"Uploaded CSV Data:\")\n",
        "    st.write(test_data.head())\n",
        "\n",
        "    # Assuming your model expects the same features as the training data\n",
        "    features = test_data.drop(\"label\", axis=1, errors=\"ignore\")  # Adjust if 'label' exists\n",
        "\n",
        "    # Button to make predictions on the uploaded data\n",
        "    if st.button(\"Make Prediction\"):\n",
        "        predictions = make_prediction(features)\n",
        "        test_data[\"Predictions\"] = predictions\n",
        "\n",
        "        # Show the predictions\n",
        "        st.write(\"Predictions:\")\n",
        "        st.write(test_data)\n",
        "\n",
        "        # Option to download the predictions as CSV\n",
        "        csv = test_data.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\n",
        "            \"Download Predictions as CSV\",\n",
        "            csv,\n",
        "            \"predictions.csv\",\n",
        "            \"text/csv\",\n",
        "            key=\"download-csv\"\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9260f401923fb5c4108c543a7d176de9733d378b3752e49535ad7c43c2271b65"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}