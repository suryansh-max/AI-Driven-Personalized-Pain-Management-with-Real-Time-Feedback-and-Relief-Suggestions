{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/pullupsboth.mp4'\n",
    "output_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/annotated_frames/af7'\n",
    "coordinates_folder = '/Users/suryanshpatel/Projects/Pose_detection/Frames/plots/co7'\n",
    "output_video_path = '/Users/suryanshpatel/Projects/Pose_detection/videos/pullUps_7.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main code in next 2 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_key_frame(prev_frame, current_frame, threshold=30):\n",
    "    diff = cv2.absdiff(prev_frame, current_frame)\n",
    "    non_zero_count = np.count_nonzero(diff)\n",
    "    return non_zero_count > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 files\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# List to store all joint points\n",
    "all_joint_points = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Capture joint points\n",
    "    joint_points = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            joint_points.append((landmark.x, landmark.y))\n",
    "\n",
    "    all_joint_points.append(joint_points)\n",
    "\n",
    "\n",
    "    # Convert the image back to BGR for rendering\n",
    "    image = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw the pose annotation on the image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(image)\n",
    "\n",
    "    # Optionally display the image (comment out if not needed)\n",
    "    cv2.imshow('Pose Detection', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_joint_points = np.array(all_joint_points)\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(key_frames)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "195: The number of frames in the video.\n",
    "480: The height of each frame in pixels.\n",
    "270: The width of each frame in pixels.\n",
    "2: The number of color channels (typically RGB, where 3 channels represent Red, Green, and Blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.array(all_joint_points)\n",
    "print(arr2.shape)\n",
    "print(arr2[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "195: Number of frames in your video.\n",
    "33: Number of joints detected in each frame.\n",
    "2: Each joint has 2 coordinates, typically representing (x, y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saves video skeleton\n",
    "saves frames - frame of every picture\n",
    "saves plots - plots for given frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and save joint coordinates\n",
    "def plot_coordinates(coordinates, frame_number, frame_width, frame_height, save_folder):\n",
    "    plt.figure()\n",
    "    \n",
    "    # Scale coordinates to frame dimensions\n",
    "    scaled_coordinates = [(x * frame_width, y * frame_height) for x, y in coordinates]\n",
    "\n",
    "    # Plot scaled coordinates\n",
    "    xs, ys = zip(*scaled_coordinates)\n",
    "    plt.plot(xs, ys, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Joint Coordinates - Frame {frame_number}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.xlim(0, frame_width)\n",
    "    plt.ylim(frame_height, 0)  # Invert y-axis limits\n",
    "    plt.savefig(os.path.join(save_folder, f'coordinates_frame_{frame_number:03d}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folders if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(coordinates_folder, exist_ok=True)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize VideoWriter for annotated video\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize variables for key frames\n",
    "key_frames = []\n",
    "prev_frame = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is None or is_key_frame(prev_frame, gray_frame):\n",
    "        key_frames.append(frame)\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Process the image and detect the pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Create a blank image (white background)\n",
    "    blank_image = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Draw the pose annotation on the blank image\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(blank_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Save annotated frame\n",
    "        annotated_frame_path = os.path.join(output_folder, f'frame_{len(key_frames):03d}.png')\n",
    "        cv2.imwrite(annotated_frame_path, cv2.cvtColor(blank_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Plot and save coordinates\n",
    "        if results.pose_landmarks:\n",
    "            joint_coordinates = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]\n",
    "            plot_coordinates(joint_coordinates, frame_number=len(key_frames), \n",
    "                             frame_width=frame_width, frame_height=frame_height, save_folder=coordinates_folder)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(blank_image)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()  # Uncomment if you are displaying the video\n",
    "\n",
    "print(f'Output video saved as {output_video_path}')\n",
    "print(f'Annotated frames saved in {output_folder}')\n",
    "print(f'Coordinate plots saved in {coordinates_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatic exercise detection attepmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_joint_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_joint_points[0][28]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
